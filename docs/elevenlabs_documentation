---
title: Developer quickstart
subtitle: Learn how to make your first ElevenLabs API request.
---

The ElevenLabs API provides a simple interface to state-of-the-art audio [models](/docs/overview/models) and [features](/docs/api-reference/introduction). Follow this guide to learn how to create lifelike speech with our Text to Speech API. See the [developer guides](/docs/developers/quickstart#explore-our-developer-guides) for more examples with our other products.

## Using the Text to Speech API

<Steps>
    <Step title="Create an API key">
      [Create an API key in the dashboard here](https://elevenlabs.io/app/settings/api-keys), which you’ll use to securely [access the API](/docs/api-reference/authentication).
      
      Store the key as a managed secret and pass it to the SDKs either as a environment variable via an `.env` file, or directly in your app’s configuration depending on your preference.
      
      ```js title=".env"
      ELEVENLABS_API_KEY=<your_api_key_here>
      ```
      
    </Step>
    <Step title="Install the SDK">
      We'll also use the `dotenv` library to load our API key from an environment variable.
      
      <CodeBlocks>
          ```python
          pip install elevenlabs
          pip install python-dotenv
          ```
      
          ```typescript
          npm install @elevenlabs/elevenlabs-js
          npm install dotenv
          ```
      
      </CodeBlocks>
      

      <Note>
        To play the audio through your speakers, you may be prompted to install [MPV](https://mpv.io/)
      and/or [ffmpeg](https://ffmpeg.org/).
      </Note>
    </Step>
    <Step title="Make your first request">
      Create a new file named `example.py` or `example.mts`, depending on your language of choice and add the following code:
       {/* This snippet was auto-generated */}
       <CodeBlocks>
       ```python
       from dotenv import load_dotenv
       from elevenlabs.client import ElevenLabs
       from elevenlabs.play import play
       import os
       
       load_dotenv()
       
       elevenlabs = ElevenLabs(
         api_key=os.getenv("ELEVENLABS_API_KEY"),
       )
       
       audio = elevenlabs.text_to_speech.convert(
           text="The first move is what sets everything in motion.",
           voice_id="JBFqnCBsd6RMkjVDRZzb",
           model_id="eleven_multilingual_v2",
           output_format="mp3_44100_128",
       )
       
       play(audio)
       
       ```
       
       ```typescript
       import { ElevenLabsClient, play } from '@elevenlabs/elevenlabs-js';
       import { Readable } from 'stream';
       import 'dotenv/config';
       
       const elevenlabs = new ElevenLabsClient();
       const audio = await elevenlabs.textToSpeech.convert('JBFqnCBsd6RMkjVDRZzb', {
         text: 'The first move is what sets everything in motion.',
         modelId: 'eleven_multilingual_v2',
         outputFormat: 'mp3_44100_128',
       });
       
       const reader = audio.getReader();
       const stream = new Readable({
         async read() {
           const { done, value } = await reader.read();
           if (done) {
             this.push(null);
           } else {
             this.push(value);
           }
         },
       });
       
       await play(stream);
       
       ```
       
       </CodeBlocks>
    </Step>
    <Step title="Run the code">
        <CodeBlocks>
            ```python
            python example.py
            ```

            ```typescript
            npx tsx example.mts
            ```
        </CodeBlocks>

        You should hear the audio play through your speakers.
    </Step>

</Steps>

## Explore our developer guides

Now that you've made your first ElevenLabs API request, you can explore the other products that ElevenLabs offers.

<CardGroup cols={2}>
  <Card
    title="Speech to Text"
    icon="duotone pen-clip"
    href="/docs/developers/guides/cookbooks/speech-to-text/quickstart"
  >
    Convert spoken audio into text
  </Card>
  <Card
    title="Music"
    icon="duotone music"
    href="/docs/developers/guides/cookbooks/music/quickstart"
  >
    Generate studio-quality music
  </Card>
  <Card
    title="Text to Dialogue"
    icon="duotone comment-dots"
    href="/docs/developers/guides/cookbooks/text-to-dialogue"
  >
    Create natural-sounding dialogue from text
  </Card>
  <Card
    title="Voice Changer"
    icon="duotone message-pen"
    href="/docs/developers/guides/cookbooks/voice-changer"
  >
    Transform the voice of an audio file
  </Card>
  <Card
    title="Voice Isolator"
    icon="duotone ear"
    href="/docs/developers/guides/cookbooks/voice-isolator"
  >
    Isolate background noise from audio
  </Card>
  <Card title="Dubbing" icon="duotone language" href="/docs/developers/guides/cookbooks/dubbing">
    Dub audio/video from one language to another
  </Card>
  <Card
    title="Sound Effects"
    icon="duotone explosion"
    href="/docs/developers/guides/cookbooks/sound-effects"
  >
    Generate sound effects from text
  </Card>
  <Card
    title="Voice Cloning"
    icon="duotone clone"
    href="/docs/developers/guides/cookbooks/voices/instant-voice-cloning"
  >
    Clone a voice
  </Card>
  <Card
    title="Voice Remixing"
    icon="duotone shuffle"
    href="/docs/developers/guides/cookbooks/voices/remix-a-voice"
  >
    Remix a voice
  </Card>

<Card
title="Voice Design"
icon="duotone paint-brush"
href="/docs/developers/guides/cookbooks/voices/voice-design"
>

    Generate voices from a single text prompt

  </Card>
  <Card
    title="Forced Alignment"
    icon="duotone objects-align-left"
    href="/docs/developers/guides/cookbooks/forced-alignment"
  >
    Generate time-aligned transcripts for audio
  </Card>
  <Card title="ElevenLabs Agents" icon="duotone comments" href="/docs/agents-platform/quickstart">
    Deploy conversational voice agents
  </Card>
</CardGroup>



---
title: Streaming text to speech
subtitle: Learn how to stream text into speech in Python or Node.js.
---

In this tutorial, you'll learn how to convert [text to speech](https://elevenlabs.io/text-to-speech) with the ElevenLabs SDK. We’ll start by talking through how to generate speech and receive a file and then how to generate speech and stream the response back. Finally, as a bonus we’ll show you how to upload the generated audio to an AWS S3 bucket, and share it through a signed URL. This signed URL will provide temporary access to the audio file, making it perfect for sharing with users by SMS or embedding into an application.

If you want to jump straight to an example you can find them in the [Python](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/python) and [Node.js](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/node) example repositories.

## Requirements

- An ElevenLabs account with an API key (here's how to [find your API key](/docs/developers/quickstart#authentication)).
- Python or Node installed on your machine
- (Optionally) an AWS account with access to S3.

## Setup

### Installing our SDK

Before you begin, make sure you have installed the necessary SDKs and libraries. You will need the ElevenLabs SDK for the text to speech conversion. You can install it using pip:

<CodeGroup>

```bash Python
pip install elevenlabs
```

```bash TypeScript
npm install @elevenlabs/elevenlabs-js
```

</CodeGroup>

Additionally, install necessary packages to manage your environmental variables:

<CodeGroup>

```bash Python
pip install python-dotenv
```

```bash TypeScript
npm install dotenv
npm install @types/dotenv --save-dev
```

</CodeGroup>

Next, create a `.env` file in your project directory and fill it with your credentials like so:

```bash .env
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here
```

## Convert text to speech (file)

To convert text to speech and save it as a file, we’ll use the `convert` method of the ElevenLabs SDK and then it locally as a `.mp3` file.

<CodeGroup>

```python Python

import os
import uuid
from dotenv import load_dotenv
from elevenlabs import VoiceSettings
from elevenlabs.client import ElevenLabs

load_dotenv()

ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
elevenlabs = ElevenLabs(
    api_key=ELEVENLABS_API_KEY,
)


def text_to_speech_file(text: str) -> str:
    # Calling the text_to_speech conversion API with detailed parameters
    response = elevenlabs.text_to_speech.convert(
        voice_id="pNInz6obpgDQGcFmaJgB", # Adam pre-made voice
        output_format="mp3_22050_32",
        text=text,
        model_id="eleven_turbo_v2_5", # use the turbo model for low latency
        # Optional voice settings that allow you to customize the output
        voice_settings=VoiceSettings(
            stability=0.0,
            similarity_boost=1.0,
            style=0.0,
            use_speaker_boost=True,
            speed=1.0,
        ),
    )

    # uncomment the line below to play the audio back
    # play(response)

    # Generating a unique file name for the output MP3 file
    save_file_path = f"{uuid.uuid4()}.mp3"

    # Writing the audio to a file
    with open(save_file_path, "wb") as f:
        for chunk in response:
            if chunk:
                f.write(chunk)

    print(f"{save_file_path}: A new audio file was saved successfully!")

    # Return the path of the saved audio file
    return save_file_path

```

```typescript TypeScript
import { ElevenLabsClient } from '@elevenlabs/elevenlabs-js';
import * as dotenv from 'dotenv';
import { createWriteStream } from 'fs';
import { v4 as uuid } from 'uuid';

dotenv.config();

const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;

const elevenlabs = new ElevenLabsClient({
  apiKey: ELEVENLABS_API_KEY,
});

export const createAudioFileFromText = async (text: string): Promise<string> => {
  return new Promise<string>(async (resolve, reject) => {
    try {
      const audio = await elevenlabs.textToSpeech.convert('JBFqnCBsd6RMkjVDRZzb', {
        modelId: 'eleven_multilingual_v2',
        text,
        outputFormat: 'mp3_44100_128',
        // Optional voice settings that allow you to customize the output
        voiceSettings: {
          stability: 0,
          similarityBoost: 0,
          useSpeakerBoost: true,
          speed: 1.0,
        },
      });

      const fileName = `${uuid()}.mp3`;
      const fileStream = createWriteStream(fileName);

      audio.pipe(fileStream);
      fileStream.on('finish', () => resolve(fileName)); // Resolve with the fileName
      fileStream.on('error', reject);
    } catch (error) {
      reject(error);
    }
  });
};
```

</CodeGroup>

You can then run this function with:

<CodeGroup>

```python Python
text_to_speech_file("Hello World")
```

```typescript TypeScript
await createAudioFileFromText('Hello World');
```

</CodeGroup>

## Convert text to speech (streaming)

If you prefer to stream the audio directly without saving it to a file, you can use our streaming feature.

<CodeGroup>

```python Python

import os
from typing import IO
from io import BytesIO
from dotenv import load_dotenv
from elevenlabs import VoiceSettings
from elevenlabs.client import ElevenLabs

load_dotenv()

ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")
elevenlabs = ElevenLabs(
    api_key=ELEVENLABS_API_KEY,
)


def text_to_speech_stream(text: str) -> IO[bytes]:
    # Perform the text-to-speech conversion
    response = elevenlabs.text_to_speech.stream(
        voice_id="pNInz6obpgDQGcFmaJgB", # Adam pre-made voice
        output_format="mp3_22050_32",
        text=text,
        model_id="eleven_multilingual_v2",
        # Optional voice settings that allow you to customize the output
        voice_settings=VoiceSettings(
            stability=0.0,
            similarity_boost=1.0,
            style=0.0,
            use_speaker_boost=True,
            speed=1.0,
        ),
    )

    # Create a BytesIO object to hold the audio data in memory
    audio_stream = BytesIO()

    # Write each chunk of audio data to the stream
    for chunk in response:
        if chunk:
            audio_stream.write(chunk)

    # Reset stream position to the beginning
    audio_stream.seek(0)

    # Return the stream for further use
    return audio_stream

```

```typescript TypeScript
import { ElevenLabsClient } from '@elevenlabs/elevenlabs-js';
import * as dotenv from 'dotenv';

dotenv.config();

const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;

if (!ELEVENLABS_API_KEY) {
  throw new Error('Missing ELEVENLABS_API_KEY in environment variables');
}

const elevenlabs = new ElevenLabsClient({
  apiKey: ELEVENLABS_API_KEY,
});

export const createAudioStreamFromText = async (text: string): Promise<Buffer> => {
  const audioStream = await elevenlabs.textToSpeech.stream('JBFqnCBsd6RMkjVDRZzb', {
    modelId: 'eleven_multilingual_v2',
    text,
    outputFormat: 'mp3_44100_128',
    // Optional voice settings that allow you to customize the output
    voiceSettings: {
      stability: 0,
      similarityBoost: 1.0,
      useSpeakerBoost: true,
      speed: 1.0,
    },
  });

  const chunks: Buffer[] = [];
  for await (const chunk of audioStream) {
    chunks.push(chunk);
  }

  const content = Buffer.concat(chunks);
  return content;
};
```

</CodeGroup>

You can then run this function with:

<CodeGroup>

```python Python
text_to_speech_stream("This is James")
```

```typescript TypeScript
await createAudioStreamFromText('This is James');
```

</CodeGroup>

## Bonus - Uploading to AWS S3 and getting a secure sharing link

Once your audio data is created as either a file or a stream you might want to share this with your users. One way to do this is to upload it to an AWS S3 bucket and generate a secure sharing link.

<AccordionGroup>
<Accordion title="Creating your AWS credentials">

To upload the data to S3 you’ll need to add your AWS access key ID, secret access key and AWS region name to your `.env` file. Follow these steps to find the credentials:

1. Log in to your AWS Management Console: Navigate to the AWS home page and sign in with your account.

<Frame caption="AWS Console Login">
  <img src="file:b768384d-b419-4aec-a392-91903d8d55ba" />
</Frame>

2. Access the IAM (Identity and Access Management) Dashboard: You can find IAM under "Security, Identity, & Compliance" on the services menu. The IAM dashboard manages access to your AWS services securely.

<Frame caption="AWS IAM Dashboard">
  <img src="file:707a8915-5207-4f1e-b339-fc3496bfa33e" />
</Frame>

3. Create a New User (if necessary): On the IAM dashboard, select "Users" and then "Add user". Enter a user name.

<Frame caption="Add AWS IAM User">
  <img src="file:7b931937-10d7-4120-a518-a9a2c1b047f1" />
</Frame>

4. Set the permissions: attach policies directly to the user according to the access level you wish to grant. For S3 uploads, you can use the AmazonS3FullAccess policy. However, it's best practice to grant least privilege, or the minimal permissions necessary to perform a task. You might want to create a custom policy that specifically allows only the necessary actions on your S3 bucket.

<Frame caption="Set Permission for AWS IAM User">
  <img src="file:e084def8-8372-4317-a709-fdd92a5e7972" />
</Frame>

5. Review and create the user: Review your settings and create the user. Upon creation, you'll be presented with an access key ID and a secret access key. Be sure to download and securely save these credentials; the secret access key cannot be retrieved again after this step.

<Frame caption="AWS Access Secret Key">
  <img src="file:8688529e-a9c9-4d25-943f-7cfcec96c8e9" />
</Frame>

6. Get AWS region name: ex. us-east-1

<Frame caption="AWS Region Name">
  <img src="file:36802694-fbb7-4409-a139-54f50973a47d" />
</Frame>

If you do not have an AWS S3 bucket, you will need to create a new one by following these steps:

1. Access the S3 dashboard: You can find S3 under "Storage" on the services menu.

<Frame caption="AWS S3 Dashboard">
  <img src="file:fd0d2427-d4a7-4b84-9b94-a079dd30601f" />
</Frame>

2. Create a new bucket: On the S3 dashboard, click the "Create bucket" button.

<Frame caption="Click Create Bucket Button">
  <img src="file:9b6b91e1-9a40-4c28-aa70-d8ffa7c814a4" />
</Frame>

3. Enter a bucket name and click on the "Create bucket" button. You can leave the other bucket options as default. The newly added bucket will appear in the list.

<Frame caption="Enter a New S3 Bucket Name">
  <img src="file:fa3ee145-a645-4085-8f48-f8dc20e74227" />
</Frame>

<Frame caption="S3 Bucket List">
  <img src="file:4020a391-c83b-4df0-9673-95f0c65d2fc0" />
</Frame>

</Accordion>
<Accordion title="Installing the AWS SDK and adding the credentials">

Install `boto3` for interacting with AWS services using `pip` and `npm`.

<CodeGroup>

```bash Python
pip install boto3
```

```bash TypeScript
npm install @aws-sdk/client-s3
npm install @aws-sdk/s3-request-presigner
```

</CodeGroup>

Then add the environment variables to `.env` file like so:

```
AWS_ACCESS_KEY_ID=your_aws_access_key_id_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key_here
AWS_REGION_NAME=your_aws_region_name_here
AWS_S3_BUCKET_NAME=your_s3_bucket_name_here
```

</Accordion>
<Accordion title="Uploading to AWS S3 and generating the signed URL">

Add the following functions to upload the audio stream to S3 and generate a signed URL.

<CodeGroup>

```python s3_uploader.py (Python)

import os
import boto3
import uuid

AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_REGION_NAME = os.getenv("AWS_REGION_NAME")
AWS_S3_BUCKET_NAME = os.getenv("AWS_S3_BUCKET_NAME")

session = boto3.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    region_name=AWS_REGION_NAME,
)
s3 = session.client("s3")


def generate_presigned_url(s3_file_name: str) -> str:
    signed_url = s3.generate_presigned_url(
        "get_object",
        Params={"Bucket": AWS_S3_BUCKET_NAME, "Key": s3_file_name},
        ExpiresIn=3600,
    )  # URL expires in 1 hour
    return signed_url


def upload_audiostream_to_s3(audio_stream) -> str:
    s3_file_name = f"{uuid.uuid4()}.mp3"  # Generates a unique file name using UUID
    s3.upload_fileobj(audio_stream, AWS_S3_BUCKET_NAME, s3_file_name)

    return s3_file_name

```

```typescript s3_uploader.ts (TypeScript)
import { S3Client, PutObjectCommand, GetObjectCommand } from '@aws-sdk/client-s3';
import { getSignedUrl } from '@aws-sdk/s3-request-presigner';
import * as dotenv from 'dotenv';
import { v4 as uuid } from 'uuid';

dotenv.config();

const { AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME, AWS_S3_BUCKET_NAME } =
  process.env;

if (!AWS_ACCESS_KEY_ID || !AWS_SECRET_ACCESS_KEY || !AWS_REGION_NAME || !AWS_S3_BUCKET_NAME) {
  throw new Error('One or more environment variables are not set. Please check your .env file.');
}

const s3 = new S3Client({
  credentials: {
    accessKeyId: AWS_ACCESS_KEY_ID,
    secretAccessKey: AWS_SECRET_ACCESS_KEY,
  },
  region: AWS_REGION_NAME,
});

export const generatePresignedUrl = async (objectKey: string) => {
  const getObjectParams = {
    Bucket: AWS_S3_BUCKET_NAME,
    Key: objectKey,
    Expires: 3600,
  };
  const command = new GetObjectCommand(getObjectParams);
  const url = await getSignedUrl(s3, command, { expiresIn: 3600 });
  return url;
};

export const uploadAudioStreamToS3 = async (audioStream: Buffer) => {
  const remotePath = `${uuid()}.mp3`;
  await s3.send(
    new PutObjectCommand({
      Bucket: AWS_S3_BUCKET_NAME,
      Key: remotePath,
      Body: audioStream,
      ContentType: 'audio/mpeg',
    })
  );
  return remotePath;
};
```

</CodeGroup>

You can then call uploading function with the audio stream from the text.

<CodeGroup>

```python Python
s3_file_name = upload_audiostream_to_s3(audio_stream)
```

```typescript TypeScript
const s3path = await uploadAudioStreamToS3(stream);
```

</CodeGroup>

After uploading the audio file to S3, generate a signed URL to share access to the file. This URL will be time-limited, meaning it will expire after a certain period, making it secure for temporary sharing.

You can now generate a URL from a file with:

<CodeGroup>

```python Python
signed_url = generate_presigned_url(s3_file_name)
print(f"Signed URL to access the file: {signed_url}")
```

```typescript TypeScript
const presignedUrl = await generatePresignedUrl(s3path);
console.log('Presigned URL:', presignedUrl);
```

</CodeGroup>

If you want to use the file multiple times, you should store the s3 file path in your database and then regenerate the signed URL each time you need rather than saving the signed URL directly as it will expire.

</Accordion>
<Accordion title="Putting it all together">

To put it all together, you can use the following script:

<CodeGroup>

```python main.py (Python)

import os

from dotenv import load_dotenv

load_dotenv()

from text_to_speech_stream import text_to_speech_stream
from s3_uploader import upload_audiostream_to_s3, generate_presigned_url


def main():
    text = "This is James"

    audio_stream = text_to_speech_stream(text)
    s3_file_name = upload_audiostream_to_s3(audio_stream)
    signed_url = generate_presigned_url(s3_file_name)

    print(f"Signed URL to access the file: {signed_url}")


if __name__ == "__main__":
    main()

```

```typescript index.ts (Typescript)
import 'dotenv/config';

import { generatePresignedUrl, uploadAudioStreamToS3 } from './s3_uploader';
import { createAudioFileFromText } from './text_to_speech_file';
import { createAudioStreamFromText } from './text_to_speech_stream';

(async () => {
  // save the audio file to disk
  const fileName = await createAudioFileFromText(
    'Today, the sky is exceptionally clear, and the sun shines brightly.'
  );

  console.log('File name:', fileName);

  // OR stream the audio, upload to S3, and get a presigned URL
  const stream = await createAudioStreamFromText(
    'Today, the sky is exceptionally clear, and the sun shines brightly.'
  );

  const s3path = await uploadAudioStreamToS3(stream);

  const presignedUrl = await generatePresignedUrl(s3path);

  console.log('Presigned URL:', presignedUrl);
})();
```

</CodeGroup>

</Accordion>
</AccordionGroup>

## Conclusion

You now know how to convert text into speech and generate a signed URL to share the audio file. This functionality opens up numerous opportunities for creating and sharing content dynamically.

Here are some examples of what you could build with this.

1. **Educational Podcasts**: Create personalized educational content that can be accessed by students on demand. Teachers can convert their lessons into audio format, upload them to S3, and share the links with students for a more engaging learning experience outside the traditional classroom setting.

2. **Accessibility Features for Websites**: Enhance website accessibility by offering text content in audio format. This can make information on websites more accessible to individuals with visual impairments or those who prefer auditory learning.

3. **Automated Customer Support Messages**: Produce automated and personalized audio messages for customer support, such as FAQs or order updates. This can provide a more engaging customer experience compared to traditional text emails.

4. **Audio Books and Narration**: Convert entire books or short stories into audio format, offering a new way for audiences to enjoy literature. Authors and publishers can diversify their content offerings and reach audiences who prefer listening over reading.

5. **Language Learning Tools**: Develop language learning aids that provide learners with audio lessons and exercises. This makes it possible to practice pronunciation and listening skills in a targeted way.

For more details, visit the following to see the full project files which give a clear structure for setting up your application:

For Python: [example repo](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/python)

For TypeScript: [example repo](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/node)

If you have any questions please create an issue on the [elevenlabs-doc Github](https://github.com/elevenlabs/elevenlabs-docs/issues).




---
title: Stitching multiple requests
subtitle: Learn how to maintain voice prosody over multiple chunks/generations.
---

When converting a large body of text into audio, you may encounter abrupt changes in prosody from one chunk to another. This can be particularly noticeable when converting text that spans multiple paragraphs or sections. In order to maintain voice prosody over multiple chunks, you can use the Request Stitching feature.

This feature allows you to provide context on what has already been generated and what will be generated in the future, helping to maintain a consistent voice and prosody throughout the entire text.

<Info>Request stitching is not available for the `eleven_v3` model.</Info>

Here's an example without Request Stitching:

<video
  controls
  className="w-full"
  src="https://eleven-public-cdn.elevenlabs.io/audio/docs/without_request_stitching.mp3"
  style={{ height: 60 }}
></video>

And the same example with Request Stitching:

<video
controls
className="w-full"
src="https://eleven-public-cdn.elevenlabs.io/audio/docs/with_request_stitching.mp3"
style={{ height: 60 }}

> </video>

## How to use Request Stitching

Request Stitching is easiest when using the ElevenLabs SDKs.

<Steps>
    <Step title="Create an API key">
        [Create an API key in the dashboard here](https://elevenlabs.io/app/settings/api-keys), which you’ll use to securely [access the API](/docs/api-reference/authentication).
        
        Store the key as a managed secret and pass it to the SDKs either as a environment variable via an `.env` file, or directly in your app’s configuration depending on your preference.
        
        ```js title=".env"
        ELEVENLABS_API_KEY=<your_api_key_here>
        ```
        
    </Step>
    <Step title="Install the SDK">
        We'll also use the `dotenv` library to load our API key from an environment variable.
        
        <CodeBlocks>
            ```python
            pip install elevenlabs
            pip install python-dotenv
            ```
        
            ```typescript
            npm install @elevenlabs/elevenlabs-js
            npm install dotenv
            ```
        
        </CodeBlocks>
        
    </Step>
    <Step title="Stitch multiple requests together">
        Create a new file named `example.py` or `example.mts`, depending on your language of choice and add the following code:

        <CodeBlocks>

            ```python
            import os
            from io import BytesIO
            from elevenlabs.client import ElevenLabs
            from elevenlabs.play import play
            from dotenv import load_dotenv

            load_dotenv()

            ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY")

            elevenlabs = ElevenLabs(
                api_key=ELEVENLABS_API_KEY,
            )

            paragraphs = [
                "The advent of technology has transformed countless sectors, with education ",
                "standing out as one of the most significantly impacted fields.",
                "In recent years, educational technology, or EdTech, has revolutionized the way ",
                "teachers deliver instruction and students absorb information.",
                "From interactive whiteboards to individual tablets loaded with educational software, ",
                "technology has opened up new avenues for learning that were previously unimaginable.",
                "One of the primary benefits of technology in education is the accessibility it provides.",
            ]

            request_ids = []
            audio_buffers = []

            for paragraph in paragraphs:
                # Usually we get back a stream from the convert function, but with_raw_response is
                # used to get the headers from the response
                with elevenlabs.text_to_speech.with_raw_response.convert(
                    text=paragraph,
                    voice_id="T7QGPtToiqH4S8VlIkMJ",
                    model_id="eleven_multilingual_v2",
                    previous_request_ids=request_ids
                ) as response:
                    request_ids.append(response._response.headers.get("request-id"))

                    # response._response.headers also contains useful information like 'character-cost',
                    # which shows the cost of the generation in characters.

                    audio_data = b''.join(chunk for chunk in response.data)
                    audio_buffers.append(BytesIO(audio_data))

            combined_stream = BytesIO(b''.join(buffer.getvalue() for buffer in audio_buffers))

            play(combined_stream)
            ```

            ```typescript
            import "dotenv/config";
            import { ElevenLabsClient, play } from "@elevenlabs/elevenlabs-js";
            import { Readable } from "node:stream";

            const elevenlabs = new ElevenLabsClient({
                apiKey: process.env.ELEVENLABS_API_KEY,
            });

            const paragraphs = [
                "The advent of technology has transformed countless sectors, with education ",
                "standing out as one of the most significantly impacted fields.",
                "In recent years, educational technology, or EdTech, has revolutionized the way ",
                "teachers deliver instruction and students absorb information.",
                "From interactive whiteboards to individual tablets loaded with educational software, ",
                "technology has opened up new avenues for learning that were previously unimaginable.",
                "One of the primary benefits of technology in education is the accessibility it provides.",
            ];

            const requestIds: string[] = [];
            const audioBuffers: Buffer[] = [];

            for (const paragraph of paragraphs) {
                // Usually we get back a stream from the convert function, but withRawResponse() is
                // used to get the headers from the response
                const response = await elevenlabs.textToSpeech
                    .convert("T7QGPtToiqH4S8VlIkMJ", {
                        text: paragraph,
                        modelId: "eleven_multilingual_v2",
                        previousRequestIds: requestIds,
                    })
                    .withRawResponse();

                // response.rawResponse.headers also contains useful information like 'character-cost',
                // which shows the cost of the generation in characters.

                requestIds.push(response.rawResponse.headers.get("request-id") ?? "");

                // Convert stream to buffer
                const chunks: Buffer[] = [];
                for await (const chunk of response.data) {
                    chunks.push(Buffer.from(chunk));
                }
                audioBuffers.push(Buffer.concat(chunks));
            }

            // Create a single readable stream from all buffers
            const combinedStream = Readable.from(Buffer.concat(audioBuffers));

            play(combinedStream);
            ```
        </CodeBlocks>
    </Step>
    <Step title="Execute the code">
        <CodeBlocks>
            ```python
            python example.py
            ```

            ```typescript
            npx tsx example.mts
            ```
        </CodeBlocks>

        You should hear the combined stitched audio play.
    </Step>

</Steps>

## FAQ

<AccordionGroup>
    <Accordion title="How does Request Stitching work with streaming?">
        In order to use the request IDs of a previous request for conditioning it needs to have processed completely. In case of streaming this means the audio has to be read completely from the response body.
    </Accordion>
    <Accordion title="How much difference does Request Stitching make?">
        The difference depends on the model, voice and voice settings used.
    </Accordion>
    <Accordion title="How old can the request IDs be?">
        The request IDs should be no older than two hours.
    </Accordion>
    <Accordion title="Is Request Stitching available on every plan?">
        Yes, unless you are an enterprise user with increased privacy requirements.
    </Accordion>

</AccordionGroup>


---
title: Streaming and Caching with Supabase
subtitle: >-
  Generate and stream speech through Supabase Edge Functions. Store speech in
  Supabase Storage and cache responses via built-in CDN.
---

## Introduction

In this tutorial you will learn how to build and edge API to generate, stream, store, and cache speech using Supabase Edge Functions, Supabase Storage, and ElevenLabs.

<iframe
  width="100%"
  height="400"
  src="https://www.youtube-nocookie.com/embed/4Roog4PAmZ8"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  allowfullscreen
></iframe>

<Tip title="Prefer to jump straight to the code?" icon="lightbulb">
  Find the [example project on
  GitHub](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/supabase/stream-and-cache-storage).
</Tip>

## Requirements

- An ElevenLabs account with an [API key](https://elevenlabs.io/app/settings/api-keys).
- A [Supabase](https://supabase.com) account (you can sign up for a free account via [database.new](https://database.new)).
- The [Supabase CLI](https://supabase.com/docs/guides/local-development) installed on your machine.
- The [Deno runtime](https://docs.deno.com/runtime/getting_started/installation/) installed on your machine and optionally [setup in your facourite IDE](https://docs.deno.com/runtime/getting_started/setup_your_environment).

## Setup

### Create a Supabase project locally

After installing the [Supabase CLI](https://supabase.com/docs/guides/local-development), run the following command to create a new Supabase project locally:

```bash
supabase init
```

### Configure the storage bucket

You can configure the Supabase CLI to automatically generate a storage bucket by adding this configuration in the `config.toml` file:

```toml ./supabase/config.toml
[storage.buckets.audio]
public = false
file_size_limit = "50MiB"
allowed_mime_types = ["audio/mp3"]
objects_path = "./audio"
```

<Note>
  Upon running `supabase start` this will create a new storage bucket in your local Supabase
  project. Should you want to push this to your hosted Supabase project, you can run `supabase seed
  buckets --linked`.
</Note>

### Configure background tasks for Supabase Edge Functions

To use background tasks in Supabase Edge Functions when developing locally, you need to add the following configuration in the `config.toml` file:

```toml ./supabase/config.toml
[edge_runtime]
policy = "per_worker"
```

<Note>
  When running with `per_worker` policy, Function won't auto-reload on edits. You will need to
  manually restart it by running `supabase functions serve`.
</Note>

### Create a Supabase Edge Function for Speech generation

Create a new Edge Function by running the following command:

```bash
supabase functions new text-to-speech
```

If you're using VS Code or Cursor, select `y` when the CLI prompts "Generate VS Code settings for Deno? [y/N]"!

### Set up the environment variables

Within the `supabase/functions` directory, create a new `.env` file and add the following variables:

```env supabase/functions/.env
# Find / create an API key at https://elevenlabs.io/app/settings/api-keys
ELEVENLABS_API_KEY=your_api_key
```

### Dependencies

The project uses a couple of dependencies:

- The [@supabase/supabase-js](https://supabase.com/docs/reference/javascript) library to interact with the Supabase database.
- The ElevenLabs [JavaScript SDK](/docs/developers/quickstart) to interact with the text-to-speech API.
- The open-source [object-hash](https://www.npmjs.com/package/object-hash) to generate a hash from the request parameters.

Since Supabase Edge Function uses the [Deno runtime](https://deno.land/), you don't need to install the dependencies, rather you can [import](https://docs.deno.com/examples/npm/) them via the `npm:` prefix.

## Code the Supabase Edge Function

In your newly created `supabase/functions/text-to-speech/index.ts` file, add the following code:

```ts supabase/functions/text-to-speech/index.ts
// Setup type definitions for built-in Supabase Runtime APIs
import 'jsr:@supabase/functions-js/edge-runtime.d.ts';
import { createClient } from 'jsr:@supabase/supabase-js@2';
import { ElevenLabsClient } from 'npm:elevenlabs';
import * as hash from 'npm:object-hash';

const supabase = createClient(
  Deno.env.get('SUPABASE_URL')!,
  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
);

const elevenlabs = new ElevenLabsClient({
  apiKey: Deno.env.get('ELEVENLABS_API_KEY'),
});

// Upload audio to Supabase Storage in a background task
async function uploadAudioToStorage(stream: ReadableStream, requestHash: string) {
  const { data, error } = await supabase.storage
    .from('audio')
    .upload(`${requestHash}.mp3`, stream, {
      contentType: 'audio/mp3',
    });

  console.log('Storage upload result', { data, error });
}

Deno.serve(async (req) => {
  // To secure your function for production, you can for example validate the request origin,
  // or append a user access token and validate it with Supabase Auth.
  console.log('Request origin', req.headers.get('host'));
  const url = new URL(req.url);
  const params = new URLSearchParams(url.search);
  const text = params.get('text');
  const voiceId = params.get('voiceId') ?? 'JBFqnCBsd6RMkjVDRZzb';

  const requestHash = hash.MD5({ text, voiceId });
  console.log('Request hash', requestHash);

  // Check storage for existing audio file
  const { data } = await supabase.storage.from('audio').createSignedUrl(`${requestHash}.mp3`, 60);

  if (data) {
    console.log('Audio file found in storage', data);
    const storageRes = await fetch(data.signedUrl);
    if (storageRes.ok) return storageRes;
  }

  if (!text) {
    return new Response(JSON.stringify({ error: 'Text parameter is required' }), {
      status: 400,
      headers: { 'Content-Type': 'application/json' },
    });
  }

  try {
    console.log('ElevenLabs API call');
    const response = await elevenlabs.textToSpeech.stream(voiceId, {
      output_format: 'mp3_44100_128',
      model_id: 'eleven_multilingual_v2',
      text,
    });

    const stream = new ReadableStream({
      async start(controller) {
        for await (const chunk of response) {
          controller.enqueue(chunk);
        }
        controller.close();
      },
    });

    // Branch stream to Supabase Storage
    const [browserStream, storageStream] = stream.tee();

    // Upload to Supabase Storage in the background
    EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash));

    // Return the streaming response immediately
    return new Response(browserStream, {
      headers: {
        'Content-Type': 'audio/mpeg',
      },
    });
  } catch (error) {
    console.log('error', { error });
    return new Response(JSON.stringify({ error: error.message }), {
      status: 500,
      headers: { 'Content-Type': 'application/json' },
    });
  }
});
```

### Code deep dive

There's a couple of things worth noting about the code. Let's step through it step by step.

<Steps>
  <Step title="Handle the incoming request">
    To handle the incoming request, use the `Deno.serve` handler. In the demo we don't validate the request origin, but you can for example validate the request origin, or append a user access token and validate it with [Supabase Auth](https://supabase.com/docs/guides/functions/auth).

    From the incoming request, the function extracts the `text` and `voiceId` parameters. The `voiceId` parameter is optional and defaults to the ElevenLabs ID for the "Allison" voice.

    Using the `object-hash` library, the function generates a hash from the request parameters. This hash is used to check for existing audio files in Supabase Storage.

    ```ts {1,5-8}
    Deno.serve(async (req) => {
    // To secure your function for production, you can for example validate the request origin,
    // or append a user access token and validate it with Supabase Auth.
    console.log("Request origin", req.headers.get("host"));
    const url = new URL(req.url);
    const params = new URLSearchParams(url.search);
    const text = params.get("text");
    const voiceId = params.get("voiceId") ?? "JBFqnCBsd6RMkjVDRZzb";

    const requestHash = hash.MD5({ text, voiceId });
    console.log("Request hash", requestHash);

    // ...
    })
    ```

  </Step>
  <Step title="Check for existing audio file in Supabase Storage">
    Supabase Storage comes with a [smart CDN built-in](https://supabase.com/docs/guides/storage/cdn/smart-cdn) allowing you to easily cache and serve your files.

    Here, the function checks for an existing audio file in Supabase Storage. If the file exists, the function returns the file from Supabase Storage.

    ```ts {4,9}
    const { data } = await supabase
      .storage
      .from("audio")
      .createSignedUrl(`${requestHash}.mp3`, 60);

    if (data) {
      console.log("Audio file found in storage", data);
      const storageRes = await fetch(data.signedUrl);
      if (storageRes.ok) return storageRes;
    }
    ```

  </Step>

  <Step title="Generate Speech as a stream and split into two branches">
    Using the streaming capabilities of the ElevenLabs API, the function generates a stream. The benefit here is that even for larger text, you can start streaming the audio back to your user immediately, and then upload the stream to Supabase Storage in the background.

    This allows for the best possible user experience, making even large text blocks feel magically quick. The magic here happens on line 17, where the `stream.tee()` method branches the readablestream into two branches: one for the browser and one for Supabase Storage.

    ```ts {1,17,20,22-27}
    try {
      const response = await elevenlabs.textToSpeech.stream(voiceId, {
        output_format: "mp3_44100_128",
        model_id: "eleven_multilingual_v2",
        text,
      });

      const stream = new ReadableStream({
        async start(controller) {
          for await (const chunk of response) {
            controller.enqueue(chunk);
          }
          controller.close();
        },
      });

      // Branch stream to Supabase Storage
      const [browserStream, storageStream] = stream.tee();

      // Upload to Supabase Storage in the background
      EdgeRuntime.waitUntil(uploadAudioToStorage(storageStream, requestHash));

      // Return the streaming response immediately
      return new Response(browserStream, {
        headers: {
          "Content-Type": "audio/mpeg",
        },
      });
    } catch (error) {
      console.log("error", { error });
      return new Response(JSON.stringify({ error: error.message }), {
        status: 500,
        headers: { "Content-Type": "application/json" },
      });
    }
    ```

  </Step>
  <Step title="Upload the audio stream to Supabase Storage in the background">
    The `EdgeRuntime.waitUntil` method on line 20 in the previous step is used to upload the audio stream to Supabase Storage in the background using the `uploadAudioToStorage` function. This allows the function to return the streaming response immediately to the browser, while the audio is being uploaded to Supabase Storage.

    Once the storage object has been created, the next time your users makes a request with the same parameters, the function will return the audio file from the Supabase Storage CDN.

    ```ts {2,8-10}
    // Upload audio to Supabase Storage in a background task
    async function uploadAudioToStorage(
      stream: ReadableStream,
      requestHash: string,
    ) {
      const { data, error } = await supabase.storage
        .from("audio")
        .upload(`${requestHash}.mp3`, stream, {
          contentType: "audio/mp3",
        });

      console.log("Storage upload result", { data, error });
    }
    ```

  </Step>
</Steps>

## Run locally

To run the function locally, run the following commands:

```bash
supabase start
```

Once the local Supabase stack is up and running, run the following command to start the function and observe the logs:

```bash
supabase functions serve
```

### Try it out

Navigate to `http://127.0.0.1:54321/functions/v1/text-to-speech?text=hello%20world` to hear the function in action.

Afterwards, navigate to `http://127.0.0.1:54323/project/default/storage/buckets/audio` to see the audio file in your local Supabase Storage bucket.

## Deploy to Supabase

If you haven't already, create a new Supabase account at [database.new](https://database.new) and link the local project to your Supabase account:

```bash
supabase link
```

Once done, run the following command to deploy the function:

```bash
supabase functions deploy
```

### Set the function secrets

Now that you have all your secrets set locally, you can run the following command to set the secrets in your Supabase project:

```bash
supabase secrets set --env-file supabase/functions/.env
```

## Test the function

The function is designed in a way that it can be used directly as a source for an `<audio>` element.

```html
<audio
  src="https://${SUPABASE_PROJECT_REF}.supabase.co/functions/v1/text-to-speech?text=Hello%2C%20world!&voiceId=JBFqnCBsd6RMkjVDRZzb"
  controls
/>
```

You can find an example frontend implementation in the complete code example on [GitHub](https://github.com/elevenlabs/elevenlabs-examples/tree/main/examples/text-to-speech/supabase/stream-and-cache-storage/src/pages/Index.tsx).
